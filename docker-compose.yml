---
version: "3.7"

services:

  spdk:
    image: ghcr.io/opiproject/opi-storage-spdk:main@sha256:314ad840617d795fbdfa1ce2ae157b53bb15ea8723ce913fd57a202762a354ec
    volumes:
      - /dev/hugepages:/dev/hugepages
      - /dev/shm:/dev/shm
      - /proc:/proc
    ports:
      - "9009:9009"
    privileged: true
    networks:
      - opi
    command: |
      sh -x -c 'sync; echo 1 > /proc/sys/vm/drop_caches  && \
            echo 1024 > /proc/sys/vm/nr_hugepages && \
            grep "" /sys/kernel/mm/hugepages/hugepages-*/nr_hugepages && \
            /usr/local/bin/spdk_tgt -m 0x1 -s 512 --no-pci 2>&1 & \
            echo wait 5s... && sleep 5s && cd /usr/libexec/spdk/scripts && \
            for i in `seq 1 10`; do ./rpc.py spdk_get_version && break || sleep 1; done  && \
            ./rpc.py bdev_malloc_create -b Malloc0 64 512 && \
            ./rpc.py bdev_malloc_create -b Malloc1 64 512 && \
            ./rpc.py nvmf_create_transport -t TCP -u 8192 -m 4 -c 0  && \
            ./rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1  && \
            ./rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 127.0.0.1 -s 4420  && \
            ./rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0 -n 1  && \
            ./rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc1 -n 2  && \
            ./rpc_http_proxy.py 0.0.0.0 9009 spdkuser spdkpass'
    healthcheck:
      test: ["CMD-SHELL", "curl --fail --insecure --user spdkuser:spdkpass -X POST -H 'Content-Type: application/json' -d '{\"id\": 1, \"method\": \"bdev_get_bdevs\"}' http://localhost:9009 || exit 1"]
      interval: 6s
      retries: 5
      start_period: 20s
      timeout: 10s

  telegraf:
    image: telegraf:1.25
    volumes:
      - ./config/telegraf-spdk.conf:/etc/telegraf/telegraf.conf:ro
    depends_on:
      - spdk
      - influxdb
    networks:
      - opi
    healthcheck:
      test: cat /proc/1/status || exit 1
      interval: 6s
      retries: 5
      start_period: 20s
      timeout: 10s

  bmc:
    image: dmtf/redfish-mockup-server:1.2.1
    networks:
      - opi

  grafana:
    image: grafana/grafana:9.3.6
    depends_on:
      - influxdb
    volumes:
      - ./config/grafana.ini:/etc/grafana/grafana.ini
      - ./config/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasource.yaml
      - ./config/grafana-dashboards.yaml:/etc/grafana/provisioning/dashboards/default.yaml
      - ./config/grafana-dashboard.json:/var/lib/grafana/dashboards/myexample.json
    ports:
      - "3000:3000"
    networks:
      - opi
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:3000/ || exit 1
      interval: 6s
      timeout: 10s
      retries: 3

  influxdb:
    image: influxdb:2.6-alpine
    ports:
      - "8086:8086"
    volumes:
      - influxdb-storage:/var/lib/influxdb
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=my-user
      - DOCKER_INFLUXDB_INIT_PASSWORD=my-password
      - DOCKER_INFLUXDB_INIT_ORG=my-org
      - DOCKER_INFLUXDB_INIT_BUCKET=my-bucket
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=YourInfluxDBAuthToken
    networks:
      - opi
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8086/ping || exit 1
      interval: 6s
      timeout: 10s
      retries: 5

  otel-gw-collector:
    image: otel/opentelemetry-collector:0.70.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml:z
    ports:
      - "1888:1888"    # pprof extension
      - "8888:8888"    # Prometheus metrics exposed by the collector
      - "8889:8889"    # Prometheus exporter metrics
      - "13133:13133"  # health_check extension
      - "4317:4317"    # OTLP gRPC receiver
      - "55679:55679"  # zpages extension
    networks:
      - opi
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8086/ping || exit 1

  prometheus:
    image: prom/prometheus:v2.41.0
    volumes:
      - ./config/prometheus.yaml:/etc/prometheus/prometheus.yml:z
    ports:
      - "9091:9090"
    networks:
      - opi
    healthcheck:
      test: ["CMD", "wget", "http://localhost:9090"]

volumes:
  influxdb-storage:

networks:
  opi:
